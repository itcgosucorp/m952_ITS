{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45b2d715",
   "metadata": {},
   "source": [
    "**<h3 style=\"text-align: center;\">INGEST DATA FROM ITS</h3>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "238de83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "import psutil\n",
    "import csv\n",
    "from datetime import datetime\n",
    "import urllib3  \n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning) # shut down warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c0432c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Limit core and thread sucessful.\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"OMP_NUM_THREADS\"] = \"8\"\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"8\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"8\"\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = \"8\"\n",
    "os.environ[\"VECLIB_MAXIMUM_THREADS\"] = \"8\"\n",
    "\n",
    "try:\n",
    "    p = psutil.Process()\n",
    "    p.cpu_affinity([0, 1, 2, 3, 4, 5, 6, 7]) \n",
    "    print(\"Limit core and thread sucessful.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error while litmiting resources: {e}\")\n",
    "\n",
    "load_dotenv()\n",
    "API_KEY_ELASTIC=os.getenv(\"ELASTIC_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658bdb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "def fetch_and_save_elastic_data(\n",
    "    index_name,\n",
    "    start_query,\n",
    "    end_query,\n",
    "    event_action,\n",
    "    event_module,\n",
    "    output_file,\n",
    "    api_key,\n",
    "):\n",
    "    url = f'https://103.9.206.216:9200/{index_name}/_search?scroll=2m'\n",
    "    headers = {\n",
    "        'Content-Type': 'application/json',\n",
    "        'Authorization': f'{api_key}'\n",
    "    }\n",
    "\n",
    "    query = {\n",
    "        \"size\": 10000,\n",
    "        \"sort\": [\"@timestamp\"],\n",
    "        \"_source\": True,\n",
    "        \"query\": {\n",
    "            \"bool\": {\n",
    "                \"must\": [\n",
    "                    {\n",
    "                        \"range\": {\n",
    "                            \"@timestamp\": {\n",
    "                                \"gte\": start_query,\n",
    "                                \"lt\": end_query,\n",
    "                                \"time_zone\": \"+07:00\"\n",
    "                            }\n",
    "                        }\n",
    "                    },\n",
    "                    {\n",
    "                        \"match\": {\n",
    "                            \"event.action\": event_action\n",
    "                        }\n",
    "                    },\n",
    "                    {\n",
    "                        \"match\": {\n",
    "                            \"event.module\": event_module\n",
    "                        }\n",
    "                    },\n",
    "                    {\n",
    "                        \"exists\": {\n",
    "                            \"field\": \"user.id\"\n",
    "                        }\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # first request\n",
    "    try:\n",
    "        response = requests.post(url, headers=headers, json=query, verify=False)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "    except Exception as e:\n",
    "        print(\"Initial fetch error:\", e)\n",
    "        return\n",
    "\n",
    "    hits = data.get('hits', {}).get('hits', [])\n",
    "    scroll_id = data.get('_scroll_id')\n",
    "\n",
    "    if os.path.exists(output_file):\n",
    "        os.remove(output_file)\n",
    "\n",
    "    def write_batch_to_csv(hits, write_header=False):\n",
    "        records = [hit['_source'] for hit in hits]\n",
    "        if not records:\n",
    "            return\n",
    "        try:\n",
    "            df = pd.json_normalize(records)\n",
    "\n",
    "            def safe_format(x):\n",
    "                if isinstance(x, list):\n",
    "                    return ','.join(map(str, x))\n",
    "                elif isinstance(x, dict):\n",
    "                    return str(x)\n",
    "                return x\n",
    "\n",
    "            df = df.applymap(safe_format)\n",
    "\n",
    "            # write file, prevent error format\n",
    "            df.to_csv(output_file,mode='a',index=False,header=write_header,encoding='utf-8',quoting=csv.QUOTE_ALL,lineterminator='\\n')\n",
    "        except Exception as e:\n",
    "            print(\"Write CSV error:\", e)\n",
    "\n",
    "    write_batch_to_csv(hits, write_header=True)\n",
    "\n",
    "    scroll_url = 'https://103.9.206.216:9200/_search/scroll'\n",
    "    while True:\n",
    "        scroll_payload = {\n",
    "            \"scroll\": \"2m\",\n",
    "            \"scroll_id\": scroll_id\n",
    "        }\n",
    "        try:\n",
    "            response = requests.post(scroll_url, headers=headers, json=scroll_payload, verify=False)\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "        except Exception as e:\n",
    "            print(\"Scroll fetch error:\", e)\n",
    "            break\n",
    "\n",
    "        hits = data.get('hits', {}).get('hits', [])\n",
    "        if not hits:\n",
    "            break\n",
    "\n",
    "        write_batch_to_csv(hits, write_header=False)\n",
    "        scroll_id = data.get('_scroll_id')\n",
    "\n",
    "    try:\n",
    "        requests.delete(\n",
    "            \"https://103.9.206.216:9200/_search/scroll\",\n",
    "            headers=headers,\n",
    "            json={\"scroll_id\": [scroll_id]},\n",
    "            verify=False\n",
    "        )\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    print(f\"âœ… Done writing '{event_action}' to {output_file} !\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26bb72ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_query = \"2025-05-15T00:00:00\" # from 17h 14/5/2025 to 17h 15/6/2025\n",
    "end_query = \"2025-06-15T23:59:59\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec2aad24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanitize_filename(ts: str) -> str:\n",
    "    return ts.replace(\":\", \"-\").replace(\"T\", \"_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9160385c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GOSU.GOSU-TT-ZION1\\AppData\\Local\\Temp\\ipykernel_22584\\1085217034.py:89: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(safe_format)\n"
     ]
    }
   ],
   "source": [
    "fetch_and_save_elastic_data(\n",
    "    index_name=\"event_gamo_m952\",\n",
    "    start_query=start_query,\n",
    "    end_query=end_query,\n",
    "    event_action=\"its_login\",\n",
    "    event_module=\"sources\",\n",
    "    output_file=f\"data/m952_login_{sanitize_filename(start_query)}_to_{sanitize_filename(end_query)}.csv\",\n",
    "    api_key=API_KEY_ELASTIC,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2463ca2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done writing 'its_purchase' to data/m952_purchase_2025-05-15_00-00-00_to_2025-06-15_23-59-59.csv !\n"
     ]
    }
   ],
   "source": [
    "fetch_and_save_elastic_data(\n",
    "    index_name=\"event_gamo_m952\",\n",
    "    start_query=start_query,\n",
    "    end_query=end_query,\n",
    "    event_action=\"its_purchase\",\n",
    "    event_module=\"sources\",\n",
    "    output_file=f\"data/m952_purchase_{sanitize_filename(start_query)}_to_{sanitize_filename(end_query)}.csv\",\n",
    "    api_key=API_KEY_ELASTIC,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8696a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done writing 'its_equipenhance' to data/m952_equipenhance_2025-05-15_00-00-00_to_2025-06-15_23-59-59.csv !\n"
     ]
    }
   ],
   "source": [
    "fetch_and_save_elastic_data(\n",
    "    index_name=\"event_gamo_m952\",\n",
    "    start_query=start_query,\n",
    "    end_query=end_query,\n",
    "    event_action=\"its_equipenhance\",\n",
    "    event_module=\"sources\",\n",
    "    output_file=f\"data/m952_equipenhance_{sanitize_filename(start_query)}_to_{sanitize_filename(end_query)}.csv\",\n",
    "    api_key=API_KEY_ELASTIC,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d08c36f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce86b3ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
